{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using the bottleneck features of a pre-trained network\n",
    "\n",
    "From: https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html\n",
    "Code: https://gist.github.com/fchollet/f35fbc80e066a49d65f1688a7e99f069\n",
    "\n",
    "A more refined approach would be to leverage a network pre-trained on a large dataset. Such a network would have already learned features that are useful for most computer vision problems, and leveraging such features would allow us to reach a better accuracy than any method that would only rely on the available data.\n",
    "\n",
    "Our strategy will be as follow: we will only instantiate the convolutional part of the model, everything up to the fully-connected layers. We will then run this model on our training and validation data once, recording the output (the \"bottleneck features\" from th VGG16 model: the last activation maps before the fully-connected layers) in two numpy arrays. Then we will train a small fully-connected model on top of the stored features.\n",
    "\n",
    "The reason why we are storing the features offline rather than adding our fully-connected model directly on top of a frozen convolutional base and running the whole thing, is computational effiency. Running VGG16 is expensive, especially if you're working on CPU, and we want to only do it once. Note that this prevents us from using data augmentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import h5py\n",
    "import numpy as np\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Convolution2D, MaxPooling2D, ZeroPadding2D\n",
    "from keras.layers import Activation, Dropout, Flatten, Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# path to the model weights file.\n",
    "weights_path = '../vgg16_weights.h5'\n",
    "top_model_weights_path = 'bottleneck_fc_model.h5'\n",
    "# dimensions of our images.\n",
    "img_width, img_height = 150, 150\n",
    "\n",
    "train_data_dir = '../data/dogs_vs_cats/train'\n",
    "validation_data_dir = '../data/dogs_vs_cats/validation'\n",
    "nb_train_samples = 3611 * 2\n",
    "nb_validation_samples = 444 * 2\n",
    "nb_epoch = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "K.set_image_dim_ordering('th')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# build the VGG16 network\n",
    "model = Sequential()\n",
    "model.add(ZeroPadding2D((1, 1), input_shape=(3, img_width, img_height)))\n",
    "\n",
    "model.add(Convolution2D(64, 3, 3, activation='relu', name='conv1_1'))\n",
    "model.add(ZeroPadding2D((1, 1)))\n",
    "model.add(Convolution2D(64, 3, 3, activation='relu', name='conv1_2'))\n",
    "model.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "\n",
    "model.add(ZeroPadding2D((1, 1)))\n",
    "model.add(Convolution2D(128, 3, 3, activation='relu', name='conv2_1'))\n",
    "model.add(ZeroPadding2D((1, 1)))\n",
    "model.add(Convolution2D(128, 3, 3, activation='relu', name='conv2_2'))\n",
    "model.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "\n",
    "model.add(ZeroPadding2D((1, 1)))\n",
    "model.add(Convolution2D(256, 3, 3, activation='relu', name='conv3_1'))\n",
    "model.add(ZeroPadding2D((1, 1)))\n",
    "model.add(Convolution2D(256, 3, 3, activation='relu', name='conv3_2'))\n",
    "model.add(ZeroPadding2D((1, 1)))\n",
    "model.add(Convolution2D(256, 3, 3, activation='relu', name='conv3_3'))\n",
    "model.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "\n",
    "model.add(ZeroPadding2D((1, 1)))\n",
    "model.add(Convolution2D(512, 3, 3, activation='relu', name='conv4_1'))\n",
    "model.add(ZeroPadding2D((1, 1)))\n",
    "model.add(Convolution2D(512, 3, 3, activation='relu', name='conv4_2'))\n",
    "model.add(ZeroPadding2D((1, 1)))\n",
    "model.add(Convolution2D(512, 3, 3, activation='relu', name='conv4_3'))\n",
    "model.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "\n",
    "model.add(ZeroPadding2D((1, 1)))\n",
    "model.add(Convolution2D(512, 3, 3, activation='relu', name='conv5_1'))\n",
    "model.add(ZeroPadding2D((1, 1)))\n",
    "model.add(Convolution2D(512, 3, 3, activation='relu', name='conv5_2'))\n",
    "model.add(ZeroPadding2D((1, 1)))\n",
    "model.add(Convolution2D(512, 3, 3, activation='relu', name='conv5_3'))\n",
    "model.add(MaxPooling2D((2, 2), strides=(2, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded.\n"
     ]
    }
   ],
   "source": [
    "# load the weights of the VGG16 networks\n",
    "# (trained on ImageNet, won the ILSVRC competition in 2014)\n",
    "# note: when there is a complete match between your model definition\n",
    "# and your weight savefile, you can simply call model.load_weights(filename)\n",
    "assert os.path.exists(weights_path), \\\n",
    "    'Model weights not found (see \"weights_path\" variable in script).'\n",
    "f = h5py.File(weights_path)\n",
    "for k in range(f.attrs['nb_layers']):\n",
    "    if k >= len(model.layers):\n",
    "        # we don't look at the last (fully-connected) layers in the savefile\n",
    "        break\n",
    "    g = f['layer_{}'.format(k)]\n",
    "    weights = [g['param_{}'.format(p)] for p in range(g.attrs['nb_params'])]\n",
    "    model.layers[k].set_weights(weights)\n",
    "f.close()\n",
    "print('Model loaded.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 7222 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "generator = datagen.flow_from_directory(\n",
    "        train_data_dir,\n",
    "        target_size=(img_width, img_height),\n",
    "        batch_size=16,\n",
    "        class_mode=None,\n",
    "        shuffle=False)\n",
    "bottleneck_features_train = model.predict_generator(generator, nb_train_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.save(open('bottleneck_features_train.npy', 'wb'), bottleneck_features_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 888 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "generator = datagen.flow_from_directory(\n",
    "        validation_data_dir,\n",
    "        target_size=(img_width, img_height),\n",
    "        batch_size=16,\n",
    "        class_mode=None,\n",
    "        shuffle=False)\n",
    "bottleneck_features_validation = model.predict_generator(generator, nb_validation_samples)\n",
    "np.save(open('bottleneck_features_validation.npy', 'wb'), bottleneck_features_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7222 samples, validate on 888 samples\n",
      "Epoch 1/20\n",
      "7222/7222 [==============================] - 15s - loss: 0.4787 - acc: 0.8225 - val_loss: 0.3627 - val_acc: 0.8412\n",
      "Epoch 2/20\n",
      "7222/7222 [==============================] - 46s - loss: 0.3100 - acc: 0.8773 - val_loss: 0.2594 - val_acc: 0.9009\n",
      "Epoch 3/20\n",
      "7222/7222 [==============================] - 58s - loss: 0.2752 - acc: 0.8883 - val_loss: 0.2743 - val_acc: 0.9167\n",
      "Epoch 4/20\n",
      "7222/7222 [==============================] - 57s - loss: 0.2558 - acc: 0.9034 - val_loss: 0.3093 - val_acc: 0.8840\n",
      "Epoch 5/20\n",
      "7222/7222 [==============================] - 59s - loss: 0.2353 - acc: 0.9119 - val_loss: 0.3173 - val_acc: 0.8919\n",
      "Epoch 6/20\n",
      "7222/7222 [==============================] - 58s - loss: 0.2157 - acc: 0.9207 - val_loss: 0.2769 - val_acc: 0.9065\n",
      "Epoch 7/20\n",
      "7222/7222 [==============================] - 60s - loss: 0.2063 - acc: 0.9219 - val_loss: 0.3296 - val_acc: 0.8806\n",
      "Epoch 8/20\n",
      "7222/7222 [==============================] - 60s - loss: 0.1923 - acc: 0.9277 - val_loss: 0.3365 - val_acc: 0.9099\n",
      "Epoch 9/20\n",
      "7222/7222 [==============================] - 61s - loss: 0.1917 - acc: 0.9304 - val_loss: 0.3881 - val_acc: 0.8851\n",
      "Epoch 10/20\n",
      "7222/7222 [==============================] - 60s - loss: 0.1743 - acc: 0.9376 - val_loss: 0.3166 - val_acc: 0.9077\n",
      "Epoch 11/20\n",
      "7222/7222 [==============================] - 61s - loss: 0.1662 - acc: 0.9434 - val_loss: 0.3118 - val_acc: 0.9043\n",
      "Epoch 12/20\n",
      "7222/7222 [==============================] - 61s - loss: 0.1523 - acc: 0.9454 - val_loss: 0.3748 - val_acc: 0.9110\n",
      "Epoch 13/20\n",
      "7222/7222 [==============================] - 59s - loss: 0.1494 - acc: 0.9439 - val_loss: 0.4122 - val_acc: 0.8773\n",
      "Epoch 14/20\n",
      "7222/7222 [==============================] - 61s - loss: 0.1385 - acc: 0.9531 - val_loss: 0.3490 - val_acc: 0.9155\n",
      "Epoch 15/20\n",
      "7222/7222 [==============================] - 61s - loss: 0.1363 - acc: 0.9511 - val_loss: 0.3885 - val_acc: 0.9133\n",
      "Epoch 16/20\n",
      "7222/7222 [==============================] - 62s - loss: 0.1325 - acc: 0.9544 - val_loss: 0.3729 - val_acc: 0.9144\n",
      "Epoch 17/20\n",
      "7222/7222 [==============================] - 60s - loss: 0.1267 - acc: 0.9578 - val_loss: 0.3660 - val_acc: 0.9122\n",
      "Epoch 18/20\n",
      "7222/7222 [==============================] - 62s - loss: 0.1110 - acc: 0.9632 - val_loss: 0.4243 - val_acc: 0.9009\n",
      "Epoch 19/20\n",
      "7222/7222 [==============================] - 63s - loss: 0.1063 - acc: 0.9651 - val_loss: 0.4382 - val_acc: 0.9077\n",
      "Epoch 20/20\n",
      "7222/7222 [==============================] - 63s - loss: 0.1058 - acc: 0.9636 - val_loss: 0.4101 - val_acc: 0.9144\n"
     ]
    }
   ],
   "source": [
    "train_data = np.load(open('bottleneck_features_train.npy', 'rb'))\n",
    "train_labels = np.array([0] * int(nb_train_samples / 2) + [1] * int(nb_train_samples / 2))\n",
    "\n",
    "validation_data = np.load(open('bottleneck_features_validation.npy', 'rb'))\n",
    "validation_labels = np.array(\n",
    "    [0] * int(nb_validation_samples / 2) + [1] * int(nb_validation_samples / 2))\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Flatten(input_shape=train_data.shape[1:]))\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model.fit(train_data, train_labels,\n",
    "          nb_epoch=nb_epoch, batch_size=16,\n",
    "          validation_data=(validation_data, validation_labels))\n",
    "model.save_weights(top_model_weights_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
