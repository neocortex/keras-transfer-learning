{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A small convnet from scratch\n",
    "\n",
    "From: https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html\n",
    "\n",
    "We will use a very small convnet with few layers and few filters per layer, alongside data augmentation and dropout. Dropout also helps reduce overfitting, by preventing a layer from seeing twice the exact same pattern, thus acting in a way analoguous to data augmentation (you could say that both dropout and data augmentation tend to disrupt random correlations occuring in your data).\n",
    "\n",
    "The code snippet below is our first model, a simple stack of 3 convolution layers with a ReLU activation and followed by max-pooling layers. This is very similar to the architectures that Yann LeCun advocated in the 1990s for image classification (with the exception of ReLU)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Convolution2D, MaxPooling2D\n",
    "from keras.layers import Activation, Dropout, Flatten, Dense\n",
    "from keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Convolution2D(32, 3, 3, input_shape=(150, 150, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Convolution2D(32, 3, 3))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Convolution2D(64, 3, 3))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# the model so far outputs 3D feature maps (height, width, features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On top of it we stick two fully-connected layers. We end the model with a single unit and a sigmoid activation, which is perfect for a binary classification. To go with it we will also use the binary_crossentropy loss to train our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.add(Flatten())  # this converts our 3D feature maps to 1D feature vectors\n",
    "model.add(Dense(64))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 7222 images belonging to 2 classes.\n",
      "Found 888 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "# this is the augmentation configuration we will use for training\n",
    "train_datagen = ImageDataGenerator(\n",
    "        rescale=1./255,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True)\n",
    "\n",
    "# this is the augmentation configuration we will use for testing:\n",
    "# only rescaling\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# this is a generator that will read pictures found in\n",
    "# subfolers of 'data/train', and indefinitely generate\n",
    "# batches of augmented image data\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "        'data/train',  # this is the target directory\n",
    "        target_size=(150, 150),  # all images will be resized to 150x150\n",
    "        batch_size=32,\n",
    "        class_mode='binary')  # since we use binary_crossentropy loss, we need binary labels\n",
    "\n",
    "# this is a similar generator, for validation data\n",
    "validation_generator = test_datagen.flow_from_directory(\n",
    "        'data/validation',\n",
    "        target_size=(150, 150),\n",
    "        batch_size=32,\n",
    "        class_mode='binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1984/2000 [============================>.] - ETA: 0s - loss: 0.7110 - acc: 0.5131"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wattx/anaconda/envs/py3/lib/python3.5/site-packages/keras/engine/training.py:1527: UserWarning: Epoch comprised more than `samples_per_epoch` samples, which might affect learning results. Set `samples_per_epoch` correctly to avoid this warning.\n",
      "  warnings.warn('Epoch comprised more than '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2016/2000 [==============================] - 124s - loss: 0.7108 - acc: 0.5119 - val_loss: 0.6919 - val_acc: 0.5288\n",
      "Epoch 2/10\n",
      "2016/2000 [==============================] - 103s - loss: 0.6878 - acc: 0.5635 - val_loss: 0.6841 - val_acc: 0.5558\n",
      "Epoch 3/10\n",
      "2016/2000 [==============================] - 102s - loss: 0.6757 - acc: 0.5784 - val_loss: 0.6554 - val_acc: 0.5995\n",
      "Epoch 4/10\n",
      "2006/2000 [==============================] - 100s - loss: 0.6653 - acc: 0.5872 - val_loss: 0.6356 - val_acc: 0.6092\n",
      "Epoch 5/10\n",
      "2016/2000 [==============================] - 104s - loss: 0.6598 - acc: 0.6017 - val_loss: 0.6324 - val_acc: 0.6226\n",
      "Epoch 6/10\n",
      "2016/2000 [==============================] - 100s - loss: 0.6442 - acc: 0.6270 - val_loss: 0.6079 - val_acc: 0.6590\n",
      "Epoch 7/10\n",
      "2016/2000 [==============================] - 102s - loss: 0.6186 - acc: 0.6553 - val_loss: 0.6016 - val_acc: 0.6481\n",
      "Epoch 8/10\n",
      "2006/2000 [==============================] - 100s - loss: 0.5843 - acc: 0.6830 - val_loss: 0.5989 - val_acc: 0.6869\n",
      "Epoch 9/10\n",
      "2016/2000 [==============================] - 101s - loss: 0.6102 - acc: 0.6637 - val_loss: 0.6011 - val_acc: 0.6638\n",
      "Epoch 10/10\n",
      "2016/2000 [==============================] - 101s - loss: 0.6228 - acc: 0.6637 - val_loss: 0.5757 - val_acc: 0.7027\n"
     ]
    }
   ],
   "source": [
    "model.fit_generator(\n",
    "        train_generator,\n",
    "        samples_per_epoch=2000,\n",
    "        nb_epoch=10,\n",
    "        validation_data=validation_generator,\n",
    "        nb_val_samples=800)\n",
    "model.save_weights('first_try.h5')  # always save your weights after training or during training"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:py3]",
   "language": "python",
   "name": "conda-env-py3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
